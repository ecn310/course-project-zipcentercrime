---------------------------------------------------------------------------------
      name:  <unnamed>
       log:  C:\Users\sgortizh\OneDrive - Syracuse University\EconResearch\course
> -project-zipcentercrime\Reproducibility Package\Downloaded_calls\Log_Files\Dist
> ances_by_500.log
  log type:  text
 opened on:  27 May 2025, 02:21:44

. 
. *** Then, import the dataset
. 
. import delimited "downloaded_calls_arcgisFile", clear
(encoding automatically selected: UTF-8)
(2 vars, 254,779 obs)

.  
. *** Then, drop any values that were further than 2500 meters from any treatment
>  center
. 
. drop if near_dist == -1
(82,598 observations deleted)

. 
. ssc install outreg2
checking outreg2 consistency and verifying not already installed...
all files already exist and are up to date.

. ssc install estout
checking estout consistency and verifying not already installed...
all files already exist and are up to date.

. 
. *** Then, create the distance rings. This is the farther bound of the distance 
> ring
. 
. gen dist_group = 500 if near_dist <= 500
(148,618 missing values generated)

. replace dist_group = 1000 if (near_dist <= 1000 & near_dist >500)
(45,011 real changes made)

. replace dist_group = 1500 if (near_dist <= 1500 & near_dist >1000)
(45,264 real changes made)

. replace dist_group = 2000 if (near_dist <= 2000 & near_dist >1500)
(33,805 real changes made)

. replace dist_group = 2500 if (near_dist <= 2500 & near_dist >2000)
(24,538 real changes made)

. 
. *** Then, create the lower bound of the distance rings. 
. 
. gen dist_group2 = 0 if near_dist <= 500 
(148,618 missing values generated)

. replace dist_group2 = 500 if (near_dist <= 1000 & near_dist >500)
(45,011 real changes made)

. replace dist_group2 = 1000 if (near_dist <= 1500 & near_dist >1000)
(45,264 real changes made)

. replace dist_group2 = 1500 if (near_dist <= 2000 & near_dist >1500)
(33,805 real changes made)

. replace dist_group2 = 2000 if (near_dist <= 2500 & near_dist >2000)
(24,538 real changes made)

. 
. 
. *dist_group2 of 2500 doesnt' have an uppderbound, so there won't be anything
. *** At this point, every call has an assigned SATC, The distance it is from tha
> t SATC, and a assigned upperbound distance ring and lowerbound distance ring. 
. ***Now, we count up the amount of observations per assigned dist_group and iden
> tify that number. 
. 
. egen freq = count(near_dist), by(dist_group)

. 
. ***Then, we found the area for each respective group
. 
. gen area = (c(pi) * dist_group^2) - (c(pi) * dist_group2^2) 

. 
. *** taking that difference in area and dividing the amount of calls in that spe
> cific distance group by the new ring of area will give us the number of calls p
> er the increase in area from one ring to the next largest, to standardize the t
> otal calls by their repestive area sizes
. 
. gen CallxArea = freq / area

. 
. ***these commands create new seperate variables for each distance group 
. gen dist_group_500 = 1 if near_dist <= 500
(148,618 missing values generated)

. gen dist_group_1000 = 1 if (near_dist <= 1000 & near_dist >500)
(127,170 missing values generated)

. gen dist_group_1500 = 1 if (near_dist <= 1500 & near_dist >1000)
(126,917 missing values generated)

. gen dist_group_2000 = 1 if (near_dist <= 2000 & near_dist >1500)
(138,376 missing values generated)

. gen dist_group_2500 = 1 if (near_dist <= 2500 & near_dist >2000)
(147,643 missing values generated)

. 
. 
. *** This is now total calls per ring. 
. ***This command collapses our data down, Using the various variables we created
>  for each seperate distance groups we can now collapse the data by the count of
>  how many of our observations are within each individual distance groups per SA
> TC center
. 
. collapse (count) dist_group_500 dist_group_1000 dist_group_1500 dist_group_2000
>  dist_group_2500, by(near_fid)

. 
. *** When collapse (count), it reduced the number of total calls to 35,731, when
>  at the beginning there was 415,216 calls. 
. 
. replace dist_group_500 = (dist_group_500 / 589048.6225) * 1000000
variable dist_group_500 was long now double
(39 real changes made)

. replace dist_group_1000 = (dist_group_1000 / 1374446.786) * 1000000
variable dist_group_1000 was long now double
(43 real changes made)

. replace dist_group_1500 = (dist_group_1500 / 2159844.949) * 1000000
variable dist_group_1500 was long now double
(42 real changes made)

. replace dist_group_2000 = (dist_group_2000 / 2945243.113) * 1000000
variable dist_group_2000 was long now double
(38 real changes made)

. replace dist_group_2500 = (dist_group_2500 / 3730641.276) * 1000000
variable dist_group_2500 was long now double
(30 real changes made)

. 
. *** To check, add up the dist~500 column to see what total calls come out
. 
. egen row_sum = rowtotal(dist_group_500 dist_group_1000 dist_group_1500 dist_gro
> up_2000 dist_group_2500)

. summarize row_sum, meanonly

. display r(sum)
111762.55

. 
. 
. ***Couldn't we also collapse count by the CallxArea variable? No, if we did, th
> en we wouldn't be taking in SATCs into the final analysis (in the unit) at all.
>  We need to do mean calls per ring per SATC center?
. ***So now, we have number of total 911 calls by ring for each SATC (1-44). How 
> do we go about doing a two sample t-test now? Either do a matrix, or this new t
> hing I will try out.
. 
. local vars dist_group_500 dist_group_1000 dist_group_1500 dist_group_2000 dist_
> group_2500

. 
. matrix Summary_Results_2 = J(1, 6, .)

. 
. *** Start the loop
. 
. foreach var of local vars {
  2.    
.    *** This is to summarize each variable 
.     summarize `var', detail
  3.     
.     *** This will save each of the needed summary stats from the Data
. 
.     local N = r(N)
  4.     local mean = r(mean)
  5.     local sd = r(sd)
  6.     local sum = r(sum)
  7.     local min = r(min)
  8.     local max = r(max)
  9.     
.         *** This adds the captured data into the Matrix
.     
.     matrix Summary_Results_2 = Summary_Results_2 \ (`N', `mean', `sd', `sum', `
> min', `max')
 10.         }

                   (count) dist_group_500
-------------------------------------------------------------
      Percentiles      Smallest
 1%            0              0
 5%            0              0
10%     164.6723              0       Obs                  43
25%     555.1324              0       Sum of wgt.          43

50%     965.9644                      Mean           930.2742
                        Largest       Std. dev.      547.0896
75%     1276.635       1629.747
90%     1614.468       1886.092       Variance         299307
95%     1886.092       2055.857       Skewness       .2726538
99%     2320.691       2320.691       Kurtosis       2.911053

                   (count) dist_group_1000
-------------------------------------------------------------
      Percentiles      Smallest
 1%     130.2342       130.2342
 5%     184.0741        172.433
10%     264.8338       184.0741       Obs                  43
25%     436.5393        244.462       Sum of wgt.          43

50%     674.4532                      Mean           761.5918
                        Largest       Std. dev.      440.2911
75%     982.2134       1471.865
90%     1429.666       1500.968       Variance       193856.3
95%     1500.968       1522.794       Skewness       .8908227
99%     2119.398       2119.398       Kurtosis       3.540046

                   (count) dist_group_1500
-------------------------------------------------------------
      Percentiles      Smallest
 1%            0              0
 5%     25.92779       18.51985
10%     44.44764       25.92779       Obs                  43
25%     205.1073       26.39078       Sum of wgt.          43

50%     437.0684                      Mean           487.3735
                        Largest       Std. dev.      375.6192
75%     702.3652       1145.916
90%      1098.69       1173.232       Variance       141089.8
95%     1173.232       1241.756       Skewness       .9970927
99%     1623.728       1623.728       Kurtosis       3.631745

                   (count) dist_group_2000
-------------------------------------------------------------
      Percentiles      Smallest
 1%            0              0
 5%            0              0
10%            0              0       Obs                  43
25%     22.40902              0       Sum of wgt.          43

50%      177.914                      Mean           266.9263
                        Largest       Std. dev.      281.7036
75%     438.6735       707.5817
90%     660.7264       809.7804       Variance       79356.94
95%     809.7804        972.755       Skewness        1.05472
99%     1039.303       1039.303       Kurtosis       3.302529

                   (count) dist_group_2500
-------------------------------------------------------------
      Percentiles      Smallest
 1%            0              0
 5%            0              0
10%            0              0       Obs                  43
25%            0              0       Sum of wgt.          43

50%      93.0135                      Mean           152.9633
                        Largest       Std. dev.      196.6004
75%     222.7499       468.0161
90%     462.9231       561.8337       Variance        38651.7
95%     561.8337       710.8697       Skewness        1.61552
99%     771.7172       771.7172       Kurtosis       4.982432

. 
.         *** Change column and row names
. matrix colnames Summary_Results_2 = "Obs" "Mean" "Std. dev." "Sum" "Min" "Max"

. 
. *** Delete empty row from matrix
. 
. matrix Summary_Results_2 = Summary_Results_2[2..6, 1..colsof(Summary_Results_2)
> ]

. 
. *** Rename the rows
. matrix rownames Summary_Results_2 = "500m" "1000m" "1500m" "2000m" "2500m"

. 
. *** Round all values to 1 decimal place
. forvalues i = 1/`=rowsof(Summary_Results)' {
  2.     forvalues j = 1/`=colsof(Summary_Results)' {
  3.         matrix Summary_Results[`i', `j'] = round(Summary_Results[`i', `j'], 
> 0.1)
  4.     }
  5. }
Summary_Results not found
invalid syntax
r(198);

end of do-file

r(198);

. do "C:\Users\sgortizh\AppData\Local\Temp\STD2b08_000000.tmp"

. *** Round all values to 1 decimal place
. forvalues i = 1/`=rowsof(Summary_Results_2)' {
  2.     forvalues j = 1/`=colsof(Summary_Results_2)' {
  3.         matrix Summary_Results_2[`i', `j'] = round(Summary_Results_2[`i', `j
> '], 0.1)
  4.     }
  5. }

. 
end of do-file

. do "C:\Users\sgortizh\AppData\Local\Temp\STD2b08_000000.tmp"

. matrix list Summary_Results_2

Summary_Results_2[5,6]
             Obs       Mean  Std. dev.        Sum        Min        Max
 500m         43      930.3      547.1    40001.8          0     2320.7
1000m         43      761.6      440.3    32748.4      130.2     2119.4
1500m         43      487.4      375.6    20957.1          0     1623.7
2000m         43      266.9      281.7    11477.8          0     1039.3
2500m         43        153      196.6     6577.4          0      771.7

. 
. *** Export table
. esttab matrix(Summary_Results_2) using "Visual_Graphics_Downloaded_calls\500_Su
> mmary_Stats.tex", replace
(output written to Visual_Graphics_Downloaded_calls\500_Summary_Stats.tex)

. 
. *** Do one-sample t-tests for every adjacent ring
. 
. * Step 1: Initialize the matrix for storing t-test results
. matrix t_tests = J(4, 4, .)  // 4 comparisons, 4 columns (Mean 1, Mean 2, Diffe
> rence, p_value)

. 
. * Run the first t-test: dist_group_500 vs. dist_group_1000
. ttest dist_group_500 == dist_group_1000

Paired t test
------------------------------------------------------------------------------
Variable |     Obs        Mean    Std. err.   Std. dev.   [95% conf. interval]
---------+--------------------------------------------------------------------
dis~_500 |      43    930.2742    83.43038    547.0896    761.9049    1098.644
dis~1000 |      43    761.5918    67.14376    440.2911    626.0902    897.0934
---------+--------------------------------------------------------------------
    diff |      43    168.6824    77.57206     508.674    12.13563    325.2291
------------------------------------------------------------------------------
     mean(diff) = mean(dist_group_500 - dist_group_1000)          t =   2.1745
 H0: mean(diff) = 0                              Degrees of freedom =       42

 Ha: mean(diff) < 0           Ha: mean(diff) != 0           Ha: mean(diff) > 0
 Pr(T < t) = 0.9823         Pr(|T| > |t|) = 0.0353          Pr(T > t) = 0.0177

. matrix t_tests[1, 1] = r(mu_1)   // Mean of dist_group_250

. matrix t_tests[1, 2] = r(mu_2)   // Mean of dist_group_500

. matrix t_tests[1, 3] = r(mu_1) - r(mu_2)  // Difference of means

. matrix t_tests[1, 4] = r(p)     // p-value

. 
. * Run the second t-test: dist_group_1000 vs. dist_group_1500
. ttest dist_group_1000 == dist_group_1500

Paired t test
------------------------------------------------------------------------------
Variable |     Obs        Mean    Std. err.   Std. dev.   [95% conf. interval]
---------+--------------------------------------------------------------------
dis~1000 |      43    761.5918    67.14376    440.2911    626.0902    897.0934
dis~1500 |      43    487.3735    57.28139    375.6192     371.775     602.972
---------+--------------------------------------------------------------------
    diff |      43    274.2183    45.47217    298.1809    182.4518    365.9849
------------------------------------------------------------------------------
     mean(diff) = mean(dist_group_1000 - dist_group_1500)         t =   6.0305
 H0: mean(diff) = 0                              Degrees of freedom =       42

 Ha: mean(diff) < 0           Ha: mean(diff) != 0           Ha: mean(diff) > 0
 Pr(T < t) = 1.0000         Pr(|T| > |t|) = 0.0000          Pr(T > t) = 0.0000

. matrix t_tests[2, 1] = r(mu_1)   // Mean of dist_group_500

. matrix t_tests[2, 2] = r(mu_2)   // Mean of dist_group_750

. matrix t_tests[2, 3] = r(mu_1) - r(mu_2)  // Difference of means

. matrix t_tests[2, 4] = r(p)     // p-value

. 
. * Run the third t-test: dist_group_1500 vs. dist_group_2000
. ttest dist_group_1500 == dist_group_2000

Paired t test
------------------------------------------------------------------------------
Variable |     Obs        Mean    Std. err.   Std. dev.   [95% conf. interval]
---------+--------------------------------------------------------------------
dis~1500 |      43    487.3735    57.28139    375.6192     371.775     602.972
dis~2000 |      43    266.9263     42.9594    281.7036    180.2307    353.6219
---------+--------------------------------------------------------------------
    diff |      43    220.4472    35.63196    233.6544     148.539    292.3554
------------------------------------------------------------------------------
     mean(diff) = mean(dist_group_1500 - dist_group_2000)         t =   6.1868
 H0: mean(diff) = 0                              Degrees of freedom =       42

 Ha: mean(diff) < 0           Ha: mean(diff) != 0           Ha: mean(diff) > 0
 Pr(T < t) = 1.0000         Pr(|T| > |t|) = 0.0000          Pr(T > t) = 0.0000

. matrix t_tests[3, 1] = r(mu_1)   // Mean of dist_group_750

. matrix t_tests[3, 2] = r(mu_2)   // Mean of dist_group_1000

. matrix t_tests[3, 3] = r(mu_1) - r(mu_2)  // Difference of means

. matrix t_tests[3, 4] = r(p)     // p-value

. 
. * Run the fourth t-test: dist_group_1000 vs. dist_group_1250
. ttest dist_group_2000 == dist_group_2500

Paired t test
------------------------------------------------------------------------------
Variable |     Obs        Mean    Std. err.   Std. dev.   [95% conf. interval]
---------+--------------------------------------------------------------------
dis~2000 |      43    266.9263     42.9594    281.7036    180.2307    353.6219
dis~2500 |      43    152.9633    29.98127    196.6004    92.45863     213.468
---------+--------------------------------------------------------------------
    diff |      43     113.963    21.82612    143.1235    69.91609    158.0099
------------------------------------------------------------------------------
     mean(diff) = mean(dist_group_2000 - dist_group_2500)         t =   5.2214
 H0: mean(diff) = 0                              Degrees of freedom =       42

 Ha: mean(diff) < 0           Ha: mean(diff) != 0           Ha: mean(diff) > 0
 Pr(T < t) = 1.0000         Pr(|T| > |t|) = 0.0000          Pr(T > t) = 0.0000

. matrix t_tests[4, 1] = r(mu_1)   // Mean of dist_group_1000

. matrix t_tests[4, 2] = r(mu_2)   // Mean of dist_group_1250

. matrix t_tests[4, 3] = r(mu_1) - r(mu_2)  // Difference of means

. matrix t_tests[4, 4] = r(p)     // p-value

. 
. * Now, name the rows and columns for the one-sample t-test table 
. matrix colnames t_tests = "Lower Ring Mean" "Upper Ring Mean" "Mean Difference"
>  "P-Value"

. matrix rownames t_tests = "500m-1000m" "1000m-1500m" "1500m-2000m" "2000m-2500m
> "

. 
. * Now, craete the tex file we will put the t-tests in 
. file open t_tests_four using "Visual_Graphics_Downloaded_calls\500_t_tests.tex"
> , write replace

. file write t_tests_four "\begin{table}[htbp]" _n

. file write t_tests_four "\centering" _n

. file write t_tests_four "\begin{tabular}{l|c c c c}" _n

. file write t_tests_four "\hline" _n

. file write t_tests_four "Comparison & Mean 1 & Mean 2 & Difference & P-value \\
> " _n

. file write t_tests_four "\hline" _n

. 
. *Now, create the local rownames for the table
. local rownames 500m-1000m 1000m-1500m 1500m-2000m 2000m-2500m

. local i = 1

. foreach row of local rownames {
  2.         local mean1 = string(el(t_tests, `i', 1), "%9.1f")
  3.         local mean2 = string(el(t_tests, `i', 2), "%9.1f")
  4.         local diff  = string(el(t_tests, `i', 3), "%9.1f")
  5.         local pval  = string(el(t_tests, `i', 4), "%9.3f")
  6.         file write t_tests_four "`row' & `mean1' & `mean2' & `diff' & `pval'
>  \\" _n
  7.         local i = `i' + 1
  8. }

. 
. file write t_tests_four "\hline" _n

. file write t_tests_four "\end{tabular}" _n

. file write t_tests_four "\caption{\textbf{One-sample T-test Results by Distance
>  Rings}}" _n

. file write t_tests_four "\label{tab:ttests}" _n

. file write t_tests_four "\end{table}" _n

. file close t_tests_four

.  
.  * Check the matrix
.  matrix list t_tests

t_tests[4,4]
             Lower Ring~n  Upper Ring~n  Mean Diffe~e       P-Value
 500m-1000m     930.27421     761.59183     168.68238     .03534852
1000m-1500m     761.59183     487.37349     274.21834     3.591e-07
1500m-2000m     487.37349     266.92628     220.44721     2.139e-07
2000m-2500m     266.92628     152.96329     113.96299     5.174e-06

. 
. * Drop this matrix to start of the CI
. matrix drop _all

. 
. **** Create two new matrices for storing Confidence Interval data
. 
. matrix lci = J(5, 1, .) 

. matrix uci = J(5, 1, .)  

. 
. *** List of variables to be groupped for looping
. 
. local groups dist_group_500 dist_group_1000 dist_group_1500 dist_group_2000 dis
> t_group_2500 

. 
. *** Begin loop by telling Stata which row to begin at
. 
. local row_index = 1

. foreach var of local groups {
  2.    *** Calculate basic statistics
.    summarize `var', detail
  3.    
.    *** Mean of the variable
.    local mean = r(mean)
  4.    
.    *** Standard deviation of the variable
.    local sd = r(sd)
  5.    
.    *** Number of observations
.    local n = r(N)
  6.    
.    *** Degrees of freedom
.    local df = `n' - 1
  7.    
.    *** Calculate standard error
.    local se = `sd' / sqrt(`n')
  8.    
.    *** Calculate t-critical value
.    scalar t_crit = invttail(`df', 0.05)
  9.    
.    *** Calculate confidence interval
.    local ci_lower = `mean' - (t_crit * `se')
 10.    local ci_upper = `mean' + (t_crit * `se')
 11.    
.    *** Display Data to check if correct
.    display "Variable: `var'"
 12.    display "Mean: `mean'"
 13.    display "Standard Deviation: `sd'"
 14.    display "Number of Observations: `n'"
 15.    display "Degrees of Freedom: `df'"
 16.    display "Standard Error: `se'"
 17.    display "CI Lower: `ci_lower'"
 18.    display "CI Upper: `ci_upper'"
 19.    
.    *** Store in matrices
.    matrix lci[`row_index',1] = `ci_lower'
 20.    matrix uci[`row_index',1] = `ci_upper'
 21.    
.    *** Increment row index
.    local row_index = `row_index' + 1
 22. }

                   (count) dist_group_500
-------------------------------------------------------------
      Percentiles      Smallest
 1%            0              0
 5%            0              0
10%     164.6723              0       Obs                  43
25%     555.1324              0       Sum of wgt.          43

50%     965.9644                      Mean           930.2742
                        Largest       Std. dev.      547.0896
75%     1276.635       1629.747
90%     1614.468       1886.092       Variance         299307
95%     1886.092       2055.857       Skewness       .2726538
99%     2320.691       2320.691       Kurtosis       2.911053
Variable: dist_group_500
Mean: 930.2742137998065
Standard Deviation: 547.0895598188516
Number of Observations: 43
Degrees of Freedom: 42
Standard Error: 83.43037571626887
CI Lower: 789.9482966794259
CI Upper: 1070.600130920187

                   (count) dist_group_1000
-------------------------------------------------------------
      Percentiles      Smallest
 1%     130.2342       130.2342
 5%     184.0741        172.433
10%     264.8338       184.0741       Obs                  43
25%     436.5393        244.462       Sum of wgt.          43

50%     674.4532                      Mean           761.5918
                        Largest       Std. dev.      440.2911
75%     982.2134       1471.865
90%     1429.666       1500.968       Variance       193856.3
95%     1500.968       1522.794       Skewness       .8908227
99%     2119.398       2119.398       Kurtosis       3.540046
Variable: dist_group_1000
Mean: 761.5918291801114
Standard Deviation: 440.2910970619326
Number of Observations: 43
Degrees of Freedom: 42
Standard Error: 67.1437628321188
CI Lower: 648.6592189953881
CI Upper: 874.5244393648347

                   (count) dist_group_1500
-------------------------------------------------------------
      Percentiles      Smallest
 1%            0              0
 5%     25.92779       18.51985
10%     44.44764       25.92779       Obs                  43
25%     205.1073       26.39078       Sum of wgt.          43

50%     437.0684                      Mean           487.3735
                        Largest       Std. dev.      375.6192
75%     702.3652       1145.916
90%      1098.69       1173.232       Variance       141089.8
95%     1173.232       1241.756       Skewness       .9970927
99%     1623.728       1623.728       Kurtosis       3.631745
Variable: dist_group_1500
Mean: 487.373486359783
Standard Deviation: 375.6192142739098
Number of Observations: 43
Degrees of Freedom: 42
Standard Error: 57.28139316622754
CI Lower: 391.0289120848218
CI Upper: 583.7180606347441

                   (count) dist_group_2000
-------------------------------------------------------------
      Percentiles      Smallest
 1%            0              0
 5%            0              0
10%            0              0       Obs                  43
25%     22.40902              0       Sum of wgt.          43

50%      177.914                      Mean           266.9263
                        Largest       Std. dev.      281.7036
75%     438.6735       707.5817
90%     660.7264       809.7804       Variance       79356.94
95%     809.7804        972.755       Skewness        1.05472
99%     1039.303       1039.303       Kurtosis       3.302529
Variable: dist_group_2000
Mean: 266.9262809673106
Standard Deviation: 281.7036415396976
Number of Observations: 43
Degrees of Freedom: 42
Standard Error: 42.95940259229243
CI Lower: 194.6706125018074
CI Upper: 339.1819494328138

                   (count) dist_group_2500
-------------------------------------------------------------
      Percentiles      Smallest
 1%            0              0
 5%            0              0
10%            0              0       Obs                  43
25%            0              0       Sum of wgt.          43

50%      93.0135                      Mean           152.9633
                        Largest       Std. dev.      196.6004
75%     222.7499       468.0161
90%     462.9231       561.8337       Variance        38651.7
95%     561.8337       710.8697       Skewness        1.61552
99%     771.7172       771.7172       Kurtosis       4.982432
Variable: dist_group_2500
Mean: 152.9632898402257
Standard Deviation: 196.6003607147899
Number of Observations: 43
Degrees of Freedom: 42
Standard Error: 29.98127393587984
CI Lower: 102.5362154638926
CI Upper: 203.3903642165588

. 
. *** Display final matrices to check if correct
. 
. matrix list lci

lci[5,1]
           c1
r1   789.9483
r2  648.65922
r3  391.02891
r4  194.67061
r5  102.53622

. matrix list uci

uci[5,1]
           c1
r1  1070.6001
r2  874.52444
r3  583.71806
r4  339.18195
r5  203.39036

. 
. *** Reshape the data to be over distance instead of treatment center  - what wa
> s this purpose?
. 
. reshape long dist_group_, i(near_fid) j(Distance)
(j = 500 1000 1500 2000 2500)

Data                               Wide   ->   Long
-----------------------------------------------------------------------------
Number of observations               43   ->   215         
Number of variables                   7   ->   4           
j variable (5 values)                     ->   Distance
xij variables:
dist_group_500 dist_group_1000 ... dist_group_2500->dist_group_
-----------------------------------------------------------------------------

. 
. *** Collapse the data to be the mean number of calls at each distance
. 
. collapse (mean) dist_group_, by(Distance)

. 
. rename dist_group mean

. 
. *** Turn the two matrices into seperate variables 
. 
. svmat lci, names(lci)

. svmat uci, names(uci)

. 
. rename mean Mean 

. 
. *** Plot the graph
. graph twoway (bar Mean Distance, lwidth(02) color(blue%40)) (rcap lci1 uci1 Dis
> tance, lcolor(black) lwidth(thin)), ytitle("Mean Calls per Km²", angle(horizont
> al))  xtitle("Distance Groups (m)", size(medsmall))  legend(label (1 "Mean Call
> s per Km^2") label(2 "95% Confidence Intervals")) graphregion(color(white)) tit
> le("Mean Calls by Distance Group with 95% CIs", size(medium))

. 
. *** Export Graph
. graph export "Visual_Graphics_Downloaded_calls\500_CI_Graph.png", replace name(
> Graph)
file Visual_Graphics_Downloaded_calls\500_CI_Graph.png saved as PNG format

. 
. 
. log close
      name:  <unnamed>
       log:  C:\Users\sgortizh\OneDrive - Syracuse University\EconResearch\course
> -project-zipcentercrime\Reproducibility Package\Downloaded_calls\Log_Files\Dist
> ances_by_500.log
  log type:  text
 closed on:  27 May 2025, 02:22:25
---------------------------------------------------------------------------------
